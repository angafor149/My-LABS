{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKXEnL6YGg/C9Wkiz0+y6W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angafor149/My-LABS/blob/main/LAB5main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zAiKf1h2byzN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Custom MAPE metric to handle edge cases\n",
        "def custom_mape(y_true, y_pred):\n",
        "    epsilon = 1e-7  # Small constant to avoid division by zero\n",
        "    diff = tf.abs((y_true - y_pred) / (y_true + epsilon))\n",
        "    # Clip the values to avoid extreme percentages\n",
        "    diff = tf.clip_by_value(diff, 0, 1)\n",
        "    return tf.reduce_mean(diff) * 100\n",
        "\n",
        "# Step 1: Load dataset with error handling\n",
        "try:\n",
        "    data = pd.read_csv('synthetic_traffic_data.csv')\n",
        "    print(f\"Loaded dataset with {len(data)} entries\")\n",
        "except FileNotFoundError:\n",
        "    raise FileNotFoundError(\"Could not find 'synthetic_traffic_data.csv'. Please check the file path.\")\n",
        "\n",
        "# Step 2: Data Preprocessing\n",
        "try:\n",
        "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
        "except ValueError as e:\n",
        "    print(f\"Error converting timestamps: {e}\")\n",
        "    print(\"Please ensure timestamp format is consistent\")\n",
        "    raise\n",
        "\n",
        "# 2.1 Extract time-based features\n",
        "data['hour'] = data['timestamp'].dt.hour\n",
        "data['day_of_week'] = data['timestamp'].dt.dayofweek\n",
        "data['is_weekend'] = data['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "# 2.2 Normalize weather data\n",
        "weather_scaler = MinMaxScaler()\n",
        "traffic_scaler = MinMaxScaler()\n",
        "\n",
        "data[['temperature', 'humidity', 'rain']] = weather_scaler.fit_transform(\n",
        "    data[['temperature', 'humidity', 'rain']].values\n",
        ")\n",
        "\n",
        "# 2.3 Create lag features\n",
        "data.sort_values(by=['intersection_id', 'timestamp'], inplace=True)\n",
        "data['traffic_flow_lag_1'] = data.groupby('intersection_id')['traffic_flow'].shift(1)\n",
        "data['traffic_flow_lag_24'] = data.groupby('intersection_id')['traffic_flow'].shift(24)\n",
        "\n",
        "# 2.4 Normalize traffic flow\n",
        "data['traffic_flow'] = traffic_scaler.fit_transform(data[['traffic_flow']])\n",
        "\n",
        "# Drop rows with NaN values\n",
        "original_len = len(data)\n",
        "data.dropna(inplace=True)\n",
        "print(f\"Dropped {original_len - len(data)} rows containing NaN values\")\n",
        "\n",
        "# Step 3: Dataset Splitting\n",
        "train_size = int(len(data) * 0.7)\n",
        "val_size = int(len(data) * 0.15)\n",
        "\n",
        "train_data = data.iloc[:train_size]\n",
        "validation_data = data.iloc[train_size:train_size + val_size]\n",
        "test_data = data.iloc[train_size + val_size:]\n",
        "\n",
        "print(f\"Training set size: {len(train_data)}\")\n",
        "print(f\"Validation set size: {len(validation_data)}\")\n",
        "print(f\"Test set size: {len(test_data)}\")\n",
        "\n",
        "features = ['intersection_id', 'hour', 'day_of_week', 'is_weekend', 'temperature',\n",
        "            'humidity', 'rain', 'traffic_flow_lag_1', 'traffic_flow_lag_24']\n",
        "target = 'traffic_flow'\n",
        "\n",
        "# Verify features\n",
        "for feature in features + [target]:\n",
        "    if feature not in data.columns:\n",
        "        raise ValueError(f\"Missing required feature: {feature}\")\n",
        "\n",
        "X_train, y_train = train_data[features].values, train_data[target].values\n",
        "X_val, y_val = validation_data[features].values, validation_data[target].values\n",
        "X_test, y_test = test_data[features].values, test_data[target].values\n",
        "\n",
        "# Feature scaling\n",
        "feature_scaler = MinMaxScaler()\n",
        "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
        "X_val_scaled = feature_scaler.transform(X_val)\n",
        "X_test_scaled = feature_scaler.transform(X_test)\n",
        "\n",
        "\n",
        "def create_sequences(X, y, timesteps):\n",
        "    \"\"\"\n",
        "    Create sequences from numpy arrays\n",
        "    \"\"\"\n",
        "    if len(X) <= timesteps:\n",
        "        raise ValueError(\"Input length must be greater than timesteps\")\n",
        "\n",
        "    X_seq = []\n",
        "    y_seq = []\n",
        "\n",
        "    for i in range(len(X) - timesteps):\n",
        "        X_seq.append(X[i:(i + timesteps)])\n",
        "        y_seq.append(y[i + timesteps])\n",
        "\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "\n",
        "timesteps = 5\n",
        "try:\n",
        "    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train, timesteps)\n",
        "    X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val, timesteps)\n",
        "    print(f\"Training sequences shape: {X_train_seq.shape}\")\n",
        "    print(f\"Validation sequences shape: {X_val_seq.shape}\")\n",
        "except ValueError as e:\n",
        "    print(f\"Error creating sequences: {e}\")\n",
        "    raise\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    LSTM(64, activation='relu', return_sequences=True,\n",
        "         input_shape=(timesteps, X_train_seq.shape[2]),\n",
        "         kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32, activation='relu',\n",
        "         kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    Dropout(0.2),\n",
        "    Dense(16, activation='relu',\n",
        "          kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "# Compile with custom MAPE\n",
        "model.compile(optimizer='adam',\n",
        "             loss='mse',\n",
        "             metrics=['mae', custom_mape])\n",
        "\n",
        "# Early stopping with more patience\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=50,  # Increased patience\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        "    min_delta=0.0001  # Minimum change to qualify as an improvement\n",
        ")\n",
        "\n",
        "# Add learning rate reduction callback\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=50,\n",
        "    min_lr=0.0001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train with both callbacks\n",
        "history = model.fit(\n",
        "    X_train_seq, y_train_seq,\n",
        "    validation_data=(X_val_seq, y_val_seq),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "val_metrics = model.evaluate(X_val_seq, y_val_seq, verbose=1)\n",
        "print(\"\\nValidation Metrics:\")\n",
        "print(f\"Loss (MSE): {val_metrics[0]:.4f}\")\n",
        "print(f\"MAE: {val_metrics[1]:.4f}\")\n",
        "print(f\"MAPE: {val_metrics[2]:.4f}%\")\n",
        "\n",
        "# Save model in the modern Keras format\n",
        "try:\n",
        "    model_path = 'traffic_prediction_model.keras'\n",
        "    model.save(model_path)\n",
        "    print(f\"\\nModel saved successfully as {model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving model: {e}\")\n",
        "\n",
        "# Print final training summary with min/max metrics\n",
        "print(\"\\nTraining Summary:\")\n",
        "print(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
        "print(f\"Best validation MAE: {min(history.history['val_mae']):.4f}\")\n",
        "print(f\"Best validation MAPE: {min(history.history['val_custom_mape']):.4f}%\")\n",
        "print(f\"\\nFinal learning rate: {tf.keras.backend.get_value(model.optimizer.learning_rate):.6f}\")\n",
        "\n",
        "# Generate predictions for validation set\n",
        "val_predictions = model.predict(X_val_seq)\n",
        "\n",
        "# Plot actual vs predicted values\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(y_val_seq[:500], val_predictions[:500], alpha=0.5)\n",
        "plt.plot([y_val_seq.min(), y_val_seq.max()], [y_val_seq.min(), y_val_seq.max()], 'r--', lw=2)\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Actual vs Predicted Traffic Flow (First 500 samples)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ]
}